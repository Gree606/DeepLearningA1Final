{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(784, 60000), y=(60000,)\n",
      "Test: X=(10000, 784), y=(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Importing all the required packages\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Loading the testing and training data and reshaping it by rolling it out.\n",
    "\n",
    "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "trainX=np.array(trainX)\n",
    "trainX=trainX.reshape(60000,784)\n",
    "testX=testX.reshape(10000,784)\n",
    "\n",
    "###########################\n",
    "y_train_encode = np.zeros((10,trainY.shape[0]))\n",
    "y_train_encode[trainY, np.array(list(range(trainY.shape[0])))] = 1\n",
    "#################\n",
    "\n",
    "#To normalize the pixel values\n",
    "trainX=trainX/255.0\n",
    "testX=testX/255.0\n",
    "trainX=np.transpose(trainX)\n",
    "trainY\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testY.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of input parameters\n",
    "n=784\n",
    "#NUmber of training examples-m\n",
    "m=60000\n",
    "#Number of neurons per hidden layer-nPerHidLayer\n",
    "nPerHidLayer=10\n",
    "#Number of output neurons=Number of outputs=Number of classes in the case of classification problem. For fashion_mnist its 10 classes.\n",
    "numClasses=10\n",
    "#Total number of layers=numLayers\n",
    "numLayers=6\n",
    "\n",
    "\n",
    "#initializing parameters as per the number of neurons in below layer(nbLayer) and front layer(nfLayer)\n",
    "def init_params(nfLayer,nbLayer):\n",
    "    W=np.random.rand(nfLayer,nbLayer)*0.01\n",
    "    B=np.random.rand(nfLayer,1)\n",
    "    return W,B\n",
    "\n",
    "#sigmoid function for all neurons except output layer\n",
    "def sigmoidFn(Z):\n",
    "    return 1/(1+np.exp(-(Z)))\n",
    "\n",
    "#softmax function for outputlayer neurons\n",
    "\n",
    "# def softmaxFn(z):\n",
    "#         numr = np.exp(z)\n",
    "#         return numr/np.sum(numr)\n",
    "\n",
    "def softmaxFn(x):\n",
    "    soft = np.zeros(x.shape)\n",
    "    for i in range(0, x.shape[1]):\n",
    "        numr = np.exp(x[:, i])\n",
    "        soft[:, i] = numr/np.sum(numr)\n",
    "    return soft\n",
    "\n",
    "#Derivative of the sigmoid function needed for back propogation.\n",
    "def dSigmoidFn(z):\n",
    "    return sigmoidFn(z)*(1-sigmoidFn(z))\n",
    "\n",
    "\n",
    "#theta gives the list of all W's and B's. theta[0] refers to the list of all W's and theta[1] refers to the list of all B'set\n",
    "#So if i want to get the 1st layer matrix of W then I give theta[0][0] as lists are indexed from 0\n",
    "def init_theta():\n",
    "    W,B=[],[]\n",
    "    w,b=init_params(nPerHidLayer,n)\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "    for k in range(2,numLayers):\n",
    "        w,b=init_params(nPerHidLayer,nPerHidLayer)\n",
    "        W.append(w)\n",
    "        B.append(b)\n",
    "    w,b=init_params(nPerHidLayer,nPerHidLayer)\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "    # print('W=',W[0].shape)\n",
    "    # print('B=',B[0].shape)\n",
    "    # print('W=',W[0].shape)\n",
    "    theta=(W,B)\n",
    "    return theta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H sum= 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def forward_propogation(theta,X):\n",
    "    # print('vals=W,B',theta[0][0].shape,theta[1][0].shape,theta[0][0])\n",
    "    H=[]\n",
    "    H.append(X)\n",
    "    # print('new X shape=',X.shape)\n",
    "    A=[]\n",
    "    for k in range(1,numLayers):\n",
    "        # print('ff nw:',(theta[0][k-1]).shape,(H[k-1]).shape,theta[1][k-1].shape)\n",
    "        a=np.matmul((theta[0][k-1]),(H[k-1]))+theta[1][k-1]\n",
    "        # print('a shape=',a[:,0])\n",
    "        A.append(a)\n",
    "        h=sigmoidFn(a)\n",
    "        # print('sigmoid=',h[0])\n",
    "        # print('a size=',a[0])\n",
    "        # print('sigmoid size=',h.shape)\n",
    "        H.append(h)\n",
    "    a=np.matmul(theta[0][numLayers-1],H[numLayers-1])+theta[1][numLayers-1]\n",
    "    A.append(a)\n",
    "    softA=softmaxFn(a)\n",
    "    H.append(softA)\n",
    "    # print ('softmax',softA.shape)\n",
    "    # print('H',H[6][:,0])\n",
    "    print('H sum=',np.sum(H[6][:,0]))\n",
    "    return A,H\n",
    "\n",
    "\n",
    "thetaI=init_theta()\n",
    "fin_Y=forward_propogation(thetaI,trainX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
