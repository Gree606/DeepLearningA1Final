{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ko4IB5IGiglb",
        "outputId": "48d1a4b6-e8b5-4558-a103-fe0f0058cf29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "!pip install wandb -q\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwo-MSGkisv1",
        "outputId": "87dfbfba-aca4-41b4-ba35-95be431a3b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Number of examples in training set\n",
            "(54000, 784)\n",
            "(10, 54000)\n",
            "(54000,)\n",
            "No of examples in validation set\n",
            "(6000, 784)\n",
            "(6000,)\n",
            "No of examples in test set\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "def getData():\n",
        "    (trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "    trainX = trainX.reshape(trainX.shape[0], -1)\n",
        "    testX = testX.reshape(testX.shape[0], -1)\n",
        "    trainX = trainX/255.0\n",
        "    testX = testX/255.0\n",
        "    \n",
        "    \n",
        "    trainX, valX, trainY, valY = train_test_split(trainX, trainY, test_size=0.1, random_state=42)\n",
        "    \n",
        "    oneHotYtrain = np.zeros((10,trainY.shape[0]))\n",
        "    oneHotYtrain[trainY, np.array(list(range(trainY.shape[0])))] = 1\n",
        "\n",
        "    oneHotYval= np.zeros((10,valY.shape[0]))\n",
        "    oneHotYval[valY, np.array(list(range(valY.shape[0])))] = 1\n",
        "\n",
        "    \n",
        "    \n",
        "    print(\"Number of examples in training set\")\n",
        "    print(trainX.shape)\n",
        "    print(oneHotYtrain.shape)\n",
        "    print(trainY.shape)\n",
        "    \n",
        "    print(\"No of examples in validation set\")\n",
        "    print(valX.shape)\n",
        "    print(valY.shape)\n",
        "    print(\"No of examples in test set\")\n",
        "    print(testX.shape)\n",
        "    print(testY.shape)\n",
        "    \n",
        "    return trainX.T, oneHotYtrain, trainY.T ,valX.T, valY.T, testX.T, testY.T, oneHotYval\n",
        "\n",
        "\n",
        "trainX, oneHotYtrain, trainY, valX, valY, testX, testY, oneHotYval = getData() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "juGkHHcjiszG"
      },
      "outputs": [],
      "source": [
        "def sigmoidFn(z):\n",
        "    return 1. / (1.+np.exp(-z))\n",
        "def reluFn(z):\n",
        "    return np.maximum(0,z)\n",
        "def tanhFn(z):\n",
        "    return np.tanh(z)\n",
        "def softmaxFn(z):\n",
        "    soft = np.zeros(z.shape)\n",
        "    for i in range(0, z.shape[1]):\n",
        "        numr = np.exp(z[:, i])\n",
        "        soft[:, i] = numr/np.sum(numr)\n",
        "    return soft\n",
        "\n",
        "#Derivatives\n",
        "def dSigmoid(z):\n",
        "    return sigmoidFn(z) * (1-sigmoidFn(z))\n",
        "def dRelu(z):\n",
        "    return 1*(z>0) \n",
        "def dTanh(z):\n",
        "    return (1 - (np.tanh(z)**2))\n",
        "\n",
        "def softmaxFn(z):\n",
        "    soft = np.zeros(z.shape)\n",
        "    for i in range(0, z.shape[1]):\n",
        "        numr = np.exp(z[:, i])\n",
        "        soft[:, i] = numr/np.sum(numr)\n",
        "    return soft\n",
        "\n",
        "def initParameters(inLayer, hidLayer, outLayer,initilisation):\n",
        "    W = []\n",
        "    B  = []\n",
        "    layers = [inLayer] + hidLayer + [outLayer]\n",
        "    for i in range(len(hidLayer)+1):\n",
        "        if initilisation == 'random':\n",
        "            W.append(np.random.rand(layers[i+1], layers[i])*0.01)\n",
        "        if initilisation == 'xavier':\n",
        "            W.append(np.random.randn(layers[i+1],layers[i])*np.sqrt(2/layers[i+1]))\n",
        "        B.append(np.random.randn(layers[i+1],1))\n",
        "    # print('W length=',len(W))\n",
        "    # print('B length=',len(B))\n",
        "    # print('W shape=',W[3].shape)\n",
        "    # print('B shape=',B[3].shape)\n",
        "    return W, B\n",
        "\n",
        "# initialize_parameters(784,[128,128,124],10,'random')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D6K0GHxCis18"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "class DLA:\n",
        "\n",
        "    def __init__(self,inLayer,hidLayer,outLayer ,initForm, activation,lossFn):\n",
        "           \n",
        "        self.inLayer   = inLayer\n",
        "        self.outLayer  = outLayer\n",
        "        self.hidLayer   = hidLayer\n",
        "        self.activation = activation\n",
        "        self.lossFn = lossFn\n",
        "        self.W    = []\n",
        "        self.B     = []\n",
        "        self.initForm = initForm\n",
        "        self.W,self.B = initParameters(self.inLayer,self.hidLayer,self.outLayer,self.initForm)\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "    def forward_propagation(self,input):\n",
        "\n",
        "        self.A = []\n",
        "        self.H  = []\n",
        "        k=0\n",
        "        \n",
        "        self.A.append(np.matmul(self.W[k],input)+self.B[k])\n",
        "        if self.activation == 'sigmoid':\n",
        "          self.H.append(sigmoidFn(self.A[k]))\n",
        "        elif self.activation == 'tanh':\n",
        "          self.H.append(tanhFn(self.A[k]))\n",
        "        elif self.activation == 'relu': \n",
        "          self.H.append(reluFn(self.A[k])) \n",
        "\n",
        "\n",
        "        for k in range(1,len(self.hidLayer)):\n",
        "            self.A.append(np.matmul(self.W[k],self.H[k-1])+self.B[k])\n",
        "            if self.activation == 'sigmoid':\n",
        "              self.H.append(sigmoidFn(self.A[k]))\n",
        "            elif self.activation == 'tanh':\n",
        "              self.H.append(tanhFn(self.A[k]))\n",
        "            elif self.activation == 'relu': \n",
        "              self.H.append(reluFn(self.A[k])) \n",
        "  \n",
        "\n",
        "\n",
        "        k=len(self.hidLayer)\n",
        "        self.A.append(np.matmul(self.W[k],self.H[k-1])+self.B[k])\n",
        "        self.H.append(softmaxFn(self.A[k]))\n",
        "\n",
        "        return self.H[-1]\n",
        "\n",
        "    def back_propagation(self,trainX,trainY):\n",
        "        dA  = [0]*(len(self.hidLayer)+1)\n",
        "        dH  = [0]*(len(self.hidLayer)+1)\n",
        "        dW  = [0]*(len(self.W))\n",
        "        dB  = [0]*(len(self.B))\n",
        "\n",
        "        n_samples = trainX.shape[1]  # Change depending on the dimensions of data\n",
        "\n",
        "\n",
        "        for k in reversed(range(len(self.hidLayer)+1)):\n",
        "            if k == len(self.hidLayer):\n",
        "              if self.lossFn == 'cross_entropy':\n",
        "                  dA[k] = self.H[k]  - trainY  # keep or remove T depending on the dimensions of data\n",
        "              elif self.lossFn == 'square_loss': \n",
        "                  dA[k] = (self.H[k] - trainY) * self.H[k] * (1 - self.H[k]) \n",
        "                \n",
        "            else:\n",
        "                dH[k] = (1/n_samples)*np.matmul(self.W[k+1].T,dA[k+1])\n",
        "                if self.activation == 'sigmoid':\n",
        "                  dA[k] = (1/n_samples)*np.multiply(dH[k],dSigmoid(self.A[k]))\n",
        "                elif self.activation == 'tanh':\n",
        "                  dA[k] = (1/n_samples)*np.multiply(dH[k],dTanh(self.A[k]))\n",
        "                elif self.activation == 'relu':\n",
        "                  dA[k] = (1/n_samples)*np.multiply(dH[k],dRelu(self.A[k]))\n",
        "                \n",
        "\n",
        "            if k == 0:\n",
        "                dW[k] = (1/n_samples)*np.matmul(dA[k],trainX.T) \n",
        "            else:\n",
        "                dW[k] = (1/n_samples)*np.matmul(dA[k],self.H[k-1].T)\n",
        "\n",
        "            dB[k]  = (1/n_samples)*np.sum(dA[k], axis=1, keepdims = True)\n",
        "        return dW,dB\n",
        "\n",
        "\n",
        "    def predict(self, X,y ):\n",
        "      output =  self.forward_propagation(X)\n",
        "      out_class=(np.argmax(output,axis=0))\n",
        "      accuracy = round(accuracy_score(y, out_class) * 100, 3)\n",
        "      return accuracy , out_class\n",
        "    def selectOpt(self,trainX,trainY,valX ,valY ,epochs,learningRate,optimiser='gd',batchSize = 64,lambd=0.0005): \n",
        "      steps = 0\n",
        "      pre_update_w = np.multiply(self.W,0)\n",
        "      pre_update_b = np.multiply(self.B,0)\n",
        "      update_w = np.multiply(self.W,0)\n",
        "      update_b = np.multiply(self.B,0)\n",
        "      vw = 0.0\n",
        "      vb = 0.0\n",
        "      eps = 1e-8\n",
        "      a1 =0.0\n",
        "      gamma = 0.9\n",
        "      beta = 0.999\n",
        "      beta1 = 0.9\n",
        "      beta2 = 0.999\n",
        "      m_t, v_t, m_hat_w, v_hat_w, m_b,v_b,m_hat_b,v_hat_b = 0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0 \n",
        "      trainAccuracy=[]\n",
        "      valAccuracy=[]\n",
        "      trainingLoss=[]\n",
        "      validationLoss=[]\n",
        "      for i in range(epochs):\n",
        "        \n",
        "\n",
        "            for batch in range(0, trainX.shape[1], batchSize):\n",
        "\n",
        "              train_b_imag =  trainX[:,batch:batch+batchSize]\n",
        "              train_l_imag =  oneHotYtrain[:,batch:batch+batchSize]\n",
        "              output =  self.forward_propagation(train_b_imag)\n",
        "              g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "              if optimiser == 'gd':\n",
        "                 self.W = self.W - np.multiply(learningRate,g_weights) - np.multiply(learningRate*lambd,self.W)\n",
        "                 self.B = self.B - np.multiply(learningRate,g_biases)\n",
        "             \n",
        "              if optimiser == 'mgd':\n",
        "                 \n",
        "                 update_w = np.multiply(gamma,pre_update_w) + np.multiply(learningRate,g_weights)\n",
        "                 self.W = self.W - update_w - np.multiply(learningRate*lambd,self.W)\n",
        "                \n",
        "                 update_b = np.multiply(gamma,pre_update_b) + np.multiply(learningRate,g_biases)\n",
        "                 self.B = self.B - update_b\n",
        "                 pre_update_w = update_w\n",
        "                 pre_update_b = update_b\n",
        "\n",
        "              if optimiser == 'ngd':\n",
        "                 \n",
        "                 cache_w = self.W\n",
        "                 cache_b = self.B\n",
        "                 self.W = self.W - np.multiply(gamma,pre_update_w)\n",
        "                 self.B = self.B - np.multiply(gamma,pre_update_b)\n",
        "                 output =  self.forward_propagation(train_b_imag)\n",
        "                 g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "\n",
        "                 update_w = np.multiply(gamma,pre_update_w) + np.multiply(learningRate,g_weights)\n",
        "                 cache_w = cache_w - update_w - np.multiply(learningRate*lambd,cache_w)\n",
        "                    \n",
        "                 update_b = np.multiply(gamma,pre_update_b) + np.multiply(learningRate,g_biases)\n",
        "                 cache_b = cache_b - update_b\n",
        "\n",
        "                 self.W = cache_w\n",
        "                 self.B = cache_b\n",
        "                 pre_update_w = update_w\n",
        "                 pre_update_b = update_b\n",
        "                \n",
        "              if optimiser == 'rmsprop': \n",
        "\n",
        "                 vw = np.multiply(vw,beta) + np.multiply(1-beta,np.power(g_weights,2))\n",
        "                 vb = np.multiply(vb,beta) + np.multiply(1-beta,np.power(g_biases,2))\n",
        "\n",
        "          \n",
        "                 update_w = np.multiply(g_weights,learningRate/np.power(vw+eps,1/2))\n",
        "                 update_b = np.multiply(g_biases,learningRate/np.power(vb+eps,1/2))\n",
        "                 \n",
        "                 self.W = self.W - update_w - np.multiply(learningRate*lambd,self.W)\n",
        "                 self.B = self.B - update_b\n",
        "                    \n",
        "              if optimiser == 'adam':\n",
        "                 m_t = np.multiply(beta1,m_t) + np.multiply(1-beta1,g_weights)\n",
        "                 v_t = np.multiply(beta2,v_t) + np.multiply(1-beta2,np.power(g_weights,2))\n",
        "                 m_b = np.multiply(beta1,m_b) + np.multiply(1-beta1,g_biases)\n",
        "                 v_b = np.multiply(beta2,v_b) + np.multiply(1-beta2,np.power(g_biases,2))\n",
        "                \n",
        "                 m_hat_w = m_t/(1 - np.power(beta1,i+1))\n",
        "                 m_hat_b = m_b/(1 - np.power(beta1,i+1))\n",
        "                \n",
        "                 v_hat_w = v_t/(1 - np.power(beta2,i+1))\n",
        "                 v_hat_b = v_b/(1 - np.power(beta2,i+1))\n",
        "                 update_w = (learningRate / np.power(v_hat_w + eps, 1/2)) * m_hat_w\n",
        "                 update_b = (learningRate / np.power(v_hat_b + eps, 1/2)) * m_hat_b\n",
        "                 self.W = self.W - update_w - np.multiply(learningRate*lambd,self.W)\n",
        "                 self.B = self.B - update_b\n",
        "\n",
        "              if optimiser == 'nadam':\n",
        "                 \n",
        "                 self.W = self.W - np.multiply(gamma,update_w)\n",
        "                 self.B  = self.B  - np.multiply(gamma,update_b)\n",
        "\n",
        "                 g_weights,g_biases = self.back_propagation(train_b_imag,train_l_imag)\n",
        "\n",
        "                 m_t =  np.multiply(beta1,m_t) + np.multiply(1 - beta1,g_weights)\n",
        "                 v_t =  np.multiply(beta2,v_t) + np.multiply(1 - beta2,np.power(g_weights, 2))\n",
        "\n",
        "                 m_b =  np.multiply(beta1,m_b) + np.multiply(1 - beta1,g_biases)\n",
        "                 v_b =  np.multiply(beta2,v_b) + np.multiply(1 - beta2,np.power(g_biases, 2))\n",
        "                \n",
        "                 m_hat_w = m_t / (1 - np.power(beta1, i+1)) \n",
        "                 v_hat_t = v_t / (1 - np.power(beta2, i+1))\n",
        "\n",
        "                 m_hat_b = m_b / (1 - np.power(beta1, i+1)) \n",
        "                 v_hat_b = v_b / (1 - np.power(beta2, i+1))\n",
        "  \n",
        "                 a1 = (1-beta1)/(1-np.power(beta,i+1))\n",
        "                 update_w = np.multiply(learningRate/(np.power(v_hat_t + eps,1/2)),(np.multiply(a1,g_weights) + np.multiply(beta1,m_hat_w)))#(a4 + a2))\n",
        "                 update_b = np.multiply(learningRate/(np.power(v_hat_b + eps,1/2)),(np.multiply(a1,g_biases)+np.multiply(beta1,m_hat_b) ))#(a5 + a3))\n",
        "                 self.W = self.W - update_w - np.multiply(learningRate*lambd,self.W)\n",
        "                 self.B = self.B - update_b\n",
        "                \n",
        "            \n",
        "            #Training loss for full dataset\n",
        "            predicted_train = self.forward_propagation(trainX)\n",
        "            predicted_train_label=(np.argmax(predicted_train,axis=0)) \n",
        "             \n",
        "            acc1 = 100*np.sum(predicted_train_label==trainY)/predicted_train.shape[1]\n",
        "            trainAccuracy.append(acc1)\n",
        "\n",
        "            predicted_val = self.forward_propagation(valX)\n",
        "            predicted_val_label=(np.argmax(predicted_val,axis=0))\n",
        "              \n",
        "            acc2 = 100*np.sum(predicted_val_label==valY)/predicted_val.shape[1]\n",
        "            valAccuracy.append(acc2)\n",
        "\n",
        "            a =self.W[1:len(self.hidLayer)]\n",
        "            b = np.sum([(np.sum((a[i]**2).reshape(1,-1))) for i in range(len(a))]) \n",
        "            if self.lossFn == 'cross_entropy':\n",
        "              trainLoss = (-np.sum(np.multiply(oneHotYtrain,np.log(predicted_train)))+((lambd/2.)*b))/trainY.shape[0]\n",
        "              valLoss = (-np.sum(np.multiply(oneHotYval,np.log(predicted_val)))+((lambd/2.)*b))/valY.shape[0]\n",
        "\n",
        "            elif self.lossFn =='square_loss':\n",
        "              trainLoss = (1/2) * np.sum((oneHotYtrain - predicted_train)**2)/trainY.shape[0]\n",
        "              valLoss =((1/2) * np.sum((oneHotYtrain- predicted_val)**2))/valY.shape[0]\n",
        "            trainingLoss.append(trainLoss)\n",
        "            validationLoss.append(valLoss)\n",
        "\n",
        "\n",
        "            print('Epoch {}: training_accuracy = {:.2f} %, training_loss= {:.5f}  ,Validation accuracy = {:.2f} ,Validation loss = {:.5f}'.format(i,acc1,trainLoss,acc2,valLoss))\n",
        "                        \n",
        "         \n",
        "      return trainAccuracy,valAccuracy,trainingLoss,validationLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xfjeSSxitAU",
        "outputId": "9fba2efe-daed-4cf0-a781-bfd7b4be382d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training_accuracy = 80.57 %, training_loss= 0.54489  ,Validation accuracy = 80.03 ,Validation loss = 0.54992\n",
            "Epoch 1: training_accuracy = 82.04 %, training_loss= 0.50133  ,Validation accuracy = 81.07 ,Validation loss = 0.51017\n",
            "Epoch 2: training_accuracy = 82.53 %, training_loss= 0.48353  ,Validation accuracy = 81.60 ,Validation loss = 0.49507\n",
            "Epoch 3: training_accuracy = 82.73 %, training_loss= 0.47384  ,Validation accuracy = 81.75 ,Validation loss = 0.49057\n",
            "Epoch 4: training_accuracy = 83.21 %, training_loss= 0.46361  ,Validation accuracy = 82.05 ,Validation loss = 0.48224\n",
            "Epoch 5: training_accuracy = 83.30 %, training_loss= 0.45758  ,Validation accuracy = 82.02 ,Validation loss = 0.47929\n",
            "Epoch 6: training_accuracy = 83.63 %, training_loss= 0.44625  ,Validation accuracy = 82.35 ,Validation loss = 0.47144\n",
            "Epoch 7: training_accuracy = 83.96 %, training_loss= 0.43833  ,Validation accuracy = 82.33 ,Validation loss = 0.46669\n",
            "Epoch 8: training_accuracy = 84.15 %, training_loss= 0.43248  ,Validation accuracy = 82.70 ,Validation loss = 0.46307\n",
            "Epoch 9: training_accuracy = 84.34 %, training_loss= 0.42635  ,Validation accuracy = 82.72 ,Validation loss = 0.46084\n",
            "Epoch 10: training_accuracy = 84.48 %, training_loss= 0.42430  ,Validation accuracy = 82.83 ,Validation loss = 0.46025\n",
            "Epoch 11: training_accuracy = 84.44 %, training_loss= 0.42340  ,Validation accuracy = 82.90 ,Validation loss = 0.46115\n",
            "Epoch 12: training_accuracy = 84.56 %, training_loss= 0.41953  ,Validation accuracy = 83.12 ,Validation loss = 0.45960\n",
            "Epoch 13: training_accuracy = 84.67 %, training_loss= 0.41660  ,Validation accuracy = 83.12 ,Validation loss = 0.45812\n",
            "Epoch 14: training_accuracy = 84.70 %, training_loss= 0.41433  ,Validation accuracy = 83.18 ,Validation loss = 0.45746\n",
            "Epoch 15: training_accuracy = 84.74 %, training_loss= 0.41289  ,Validation accuracy = 83.08 ,Validation loss = 0.45762\n",
            "Epoch 16: training_accuracy = 84.80 %, training_loss= 0.41066  ,Validation accuracy = 83.05 ,Validation loss = 0.45656\n",
            "Epoch 17: training_accuracy = 84.97 %, training_loss= 0.40680  ,Validation accuracy = 83.40 ,Validation loss = 0.45381\n",
            "Epoch 18: training_accuracy = 84.97 %, training_loss= 0.40621  ,Validation accuracy = 83.22 ,Validation loss = 0.45490\n",
            "Epoch 19: training_accuracy = 84.97 %, training_loss= 0.40513  ,Validation accuracy = 83.13 ,Validation loss = 0.45576\n"
          ]
        }
      ],
      "source": [
        "numClass = 10\n",
        "model = DLA(28*28,[128,128,128],numClass,initForm= 'xavier',activation='relu', lossFn= 'cross_entropy')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "acc1,acc2,train_loss, val_loss= model.selectOpt(trainX,trainY,valX ,valY ,learningRate = 0.01,epochs=20, optimiser='adam',batchSize =32,lambd=0.0005)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xIIl9gEVdd5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d3a29a-fcf6-4162-e9ac-a56315325395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Number of examples in training set\n",
            "(54000, 784)\n",
            "(10, 54000)\n",
            "(54000,)\n",
            "No of examples in validation set\n",
            "(6000, 784)\n",
            "(6000,)\n",
            "No of examples in test set\n",
            "(10000, 784)\n",
            "(10000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "def getDataMNIST():\n",
        "    (X, Y), (testMX, testMY) = mnist.load_data()\n",
        "\n",
        "    X = X.reshape(X.shape[0], -1)\n",
        "    testMX = testMX.reshape(testMX.shape[0], -1)\n",
        "    X = X/255.0\n",
        "    testMX = testMX/255.0\n",
        "    trainMX, valMX, trainMY, valMY = train_test_split(X, Y, test_size=0.1, random_state=42)\n",
        "   \n",
        "    oneHotTrainMY = np.zeros((10,trainMY.shape[0]))\n",
        "    oneHotTrainMY[trainMY, np.array(list(range(trainMY.shape[0])))] = 1\n",
        "\n",
        "    oneHotValMY = np.zeros((10,valMY.shape[0]))\n",
        "    oneHotValMY[valMY, np.array(list(range(valMY.shape[0])))] = 1\n",
        "\n",
        "    print(\"Number of examples in training set\")\n",
        "    print(trainMX.shape)\n",
        "    print(oneHotTrainMY.shape)\n",
        "    print(trainMY.shape)\n",
        "    print(\"No of examples in validation set\")\n",
        "    print(valMX.shape)\n",
        "    print(valMY.shape)\n",
        "    print(\"No of examples in test set\")\n",
        "    print(testMX.shape)\n",
        "    print(testMY.shape)\n",
        "   \n",
        "    return trainMX.T, oneHotTrainMY, trainMY.T ,valMX.T, valMY.T, testMX.T, testMY.T, oneHotTrainMY\n",
        "\n",
        "\n",
        "trainX,oneHotYTrain,trainY,valX,valY,testX,testY,oneHotYVal = getDataMNIST()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#configuration 1\n",
        "model1 = DLA(784,[128,128,128],numClass,initForm='xavier',activation='relu',lossFn='cross_entropy')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "acc1,acc2,trainLoss, valLoss= model1.selectOpt(trainX,trainY,valX,valY,learningRate=0.01,epochs=20,optimiser='adam',batchSize =32,lambd=0.0005)\n",
        "accuracy, testPredicted = model1.predict(testX,testY)\n",
        "print(\"Test accuracy: \",accuracy)"
      ],
      "metadata": {
        "id": "XIScDCZUBkVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e78ca3-c30a-4102-dc08-dd1a370affbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training_accuracy = 10.67 %, training_loss= 2.30914  ,Validation accuracy = 9.97 ,Validation loss = 2.31053\n",
            "Epoch 1: training_accuracy = 10.83 %, training_loss= 2.31108  ,Validation accuracy = 10.55 ,Validation loss = 2.31220\n",
            "Epoch 2: training_accuracy = 10.63 %, training_loss= 2.31035  ,Validation accuracy = 10.57 ,Validation loss = 2.31161\n",
            "Epoch 3: training_accuracy = 10.53 %, training_loss= 2.30921  ,Validation accuracy = 10.48 ,Validation loss = 2.31059\n",
            "Epoch 4: training_accuracy = 10.55 %, training_loss= 2.30819  ,Validation accuracy = 10.45 ,Validation loss = 2.30945\n",
            "Epoch 5: training_accuracy = 10.58 %, training_loss= 2.30732  ,Validation accuracy = 10.32 ,Validation loss = 2.30854\n",
            "Epoch 6: training_accuracy = 10.14 %, training_loss= 2.30648  ,Validation accuracy = 9.92 ,Validation loss = 2.30898\n",
            "Epoch 7: training_accuracy = 10.41 %, training_loss= 2.30577  ,Validation accuracy = 10.38 ,Validation loss = 2.30771\n",
            "Epoch 8: training_accuracy = 10.36 %, training_loss= 2.30525  ,Validation accuracy = 10.25 ,Validation loss = 2.30748\n",
            "Epoch 9: training_accuracy = 10.15 %, training_loss= 2.30477  ,Validation accuracy = 10.18 ,Validation loss = 2.30710\n",
            "Epoch 10: training_accuracy = 10.39 %, training_loss= 2.30438  ,Validation accuracy = 10.23 ,Validation loss = 2.30642\n",
            "Epoch 11: training_accuracy = 10.16 %, training_loss= 2.30377  ,Validation accuracy = 10.15 ,Validation loss = 2.30653\n",
            "Epoch 12: training_accuracy = 9.96 %, training_loss= 2.30358  ,Validation accuracy = 9.87 ,Validation loss = 2.30605\n",
            "Epoch 13: training_accuracy = 9.78 %, training_loss= 2.30333  ,Validation accuracy = 9.77 ,Validation loss = 2.30558\n",
            "Epoch 14: training_accuracy = 9.52 %, training_loss= 2.30331  ,Validation accuracy = 9.68 ,Validation loss = 2.30597\n",
            "Epoch 15: training_accuracy = 9.52 %, training_loss= 2.30329  ,Validation accuracy = 9.67 ,Validation loss = 2.30663\n",
            "Epoch 16: training_accuracy = 9.28 %, training_loss= 2.30318  ,Validation accuracy = 9.32 ,Validation loss = 2.30535\n",
            "Epoch 17: training_accuracy = 9.32 %, training_loss= 2.30287  ,Validation accuracy = 9.17 ,Validation loss = 2.30528\n",
            "Epoch 18: training_accuracy = 9.22 %, training_loss= 2.30288  ,Validation accuracy = 9.27 ,Validation loss = 2.30512\n",
            "Epoch 19: training_accuracy = 9.09 %, training_loss= 2.30276  ,Validation accuracy = 8.87 ,Validation loss = 2.30504\n",
            "Test accuracy:  9.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#config2\n",
        "model2 = DLA(784,[128,128,128],numClass,initForm='xavier',activation='relu',lossFn='cross_entropy')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "acc1,acc2,trainLoss, valLoss= model2.selectOpt(trainX,trainY,valX ,valY,learningRate = 0.001,epochs=20, optimiser='nadam',batchSize =32,lambd=0.0005)\n",
        "accuracy, testPredicted = model2.predict(testX,testY)\n",
        "print(\"Test accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "NWMCRKrmBmf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c26e9533-1796-4625-9857-09593753fb18"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training_accuracy = 6.78 %, training_loss= 2.30521  ,Validation accuracy = 6.83 ,Validation loss = 2.30615\n",
            "Epoch 1: training_accuracy = 6.86 %, training_loss= 2.30344  ,Validation accuracy = 7.17 ,Validation loss = 2.30438\n",
            "Epoch 2: training_accuracy = 7.07 %, training_loss= 2.30265  ,Validation accuracy = 7.02 ,Validation loss = 2.30337\n",
            "Epoch 3: training_accuracy = 7.06 %, training_loss= 2.30212  ,Validation accuracy = 7.18 ,Validation loss = 2.30293\n",
            "Epoch 4: training_accuracy = 6.01 %, training_loss= 2.30211  ,Validation accuracy = 6.00 ,Validation loss = 2.30234\n",
            "Epoch 5: training_accuracy = 5.95 %, training_loss= 2.30244  ,Validation accuracy = 6.03 ,Validation loss = 2.30261\n",
            "Epoch 6: training_accuracy = 6.59 %, training_loss= 2.30319  ,Validation accuracy = 6.55 ,Validation loss = 2.30272\n",
            "Epoch 7: training_accuracy = 6.13 %, training_loss= 2.30284  ,Validation accuracy = 5.88 ,Validation loss = 2.30270\n",
            "Epoch 8: training_accuracy = 5.68 %, training_loss= 2.30285  ,Validation accuracy = 5.77 ,Validation loss = 2.30286\n",
            "Epoch 9: training_accuracy = 5.17 %, training_loss= 2.30311  ,Validation accuracy = 5.15 ,Validation loss = 2.30350\n",
            "Epoch 10: training_accuracy = 5.90 %, training_loss= 2.30288  ,Validation accuracy = 5.60 ,Validation loss = 2.30407\n",
            "Epoch 11: training_accuracy = 5.45 %, training_loss= 2.30253  ,Validation accuracy = 5.27 ,Validation loss = 2.30477\n",
            "Epoch 12: training_accuracy = 6.49 %, training_loss= 2.30205  ,Validation accuracy = 6.60 ,Validation loss = 2.30465\n",
            "Epoch 13: training_accuracy = 6.52 %, training_loss= 2.30211  ,Validation accuracy = 6.47 ,Validation loss = 2.30517\n",
            "Epoch 14: training_accuracy = 6.19 %, training_loss= 2.30258  ,Validation accuracy = 6.25 ,Validation loss = 2.30582\n",
            "Epoch 15: training_accuracy = 6.06 %, training_loss= 2.30281  ,Validation accuracy = 6.08 ,Validation loss = 2.30646\n",
            "Epoch 16: training_accuracy = 6.05 %, training_loss= 2.30311  ,Validation accuracy = 6.03 ,Validation loss = 2.30690\n",
            "Epoch 17: training_accuracy = 6.31 %, training_loss= 2.30275  ,Validation accuracy = 6.37 ,Validation loss = 2.30714\n",
            "Epoch 18: training_accuracy = 6.37 %, training_loss= 2.30219  ,Validation accuracy = 6.47 ,Validation loss = 2.30704\n",
            "Epoch 19: training_accuracy = 6.52 %, training_loss= 2.30201  ,Validation accuracy = 6.45 ,Validation loss = 2.30718\n",
            "Test accuracy:  6.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#config3\n",
        "model3 = DLA(784,[128,128,128],numClass,initForm = 'xavier', activation='relu', lossFn = 'cross_entropy')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
        "acc1,acc2,trainLoss, valLoss= model3.selectOpt(trainX,trainY,valX,valY ,learningRate = 0.001,epochs=50, optimiser='rmsprop',batchSize =64,lambd=0.0005)\n",
        "accuracy, test_predicted = model3.predict(testX,testY)\n",
        "print(\"Test accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "AdCIY7ZWBqED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273b97ac-03ef-4ccd-f8d9-dbe34c4704ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: training_accuracy = 10.53 %, training_loss= 2.51626  ,Validation accuracy = 10.13 ,Validation loss = 2.53394\n",
            "Epoch 1: training_accuracy = 11.56 %, training_loss= 2.42724  ,Validation accuracy = 11.38 ,Validation loss = 2.44847\n",
            "Epoch 2: training_accuracy = 11.97 %, training_loss= 2.39223  ,Validation accuracy = 11.83 ,Validation loss = 2.41554\n",
            "Epoch 3: training_accuracy = 11.97 %, training_loss= 2.37320  ,Validation accuracy = 11.85 ,Validation loss = 2.39778\n",
            "Epoch 4: training_accuracy = 11.86 %, training_loss= 2.36069  ,Validation accuracy = 11.78 ,Validation loss = 2.38590\n",
            "Epoch 5: training_accuracy = 11.94 %, training_loss= 2.35191  ,Validation accuracy = 11.93 ,Validation loss = 2.37740\n",
            "Epoch 6: training_accuracy = 11.96 %, training_loss= 2.34504  ,Validation accuracy = 11.83 ,Validation loss = 2.37095\n",
            "Epoch 7: training_accuracy = 11.97 %, training_loss= 2.33955  ,Validation accuracy = 11.85 ,Validation loss = 2.36588\n",
            "Epoch 8: training_accuracy = 11.93 %, training_loss= 2.33490  ,Validation accuracy = 11.67 ,Validation loss = 2.36162\n",
            "Epoch 9: training_accuracy = 11.95 %, training_loss= 2.33119  ,Validation accuracy = 11.60 ,Validation loss = 2.35826\n",
            "Epoch 10: training_accuracy = 11.92 %, training_loss= 2.32778  ,Validation accuracy = 11.48 ,Validation loss = 2.35529\n",
            "Epoch 11: training_accuracy = 11.92 %, training_loss= 2.32506  ,Validation accuracy = 11.48 ,Validation loss = 2.35303\n",
            "Epoch 12: training_accuracy = 12.01 %, training_loss= 2.32258  ,Validation accuracy = 11.47 ,Validation loss = 2.35105\n",
            "Epoch 13: training_accuracy = 12.02 %, training_loss= 2.32051  ,Validation accuracy = 11.68 ,Validation loss = 2.34941\n",
            "Epoch 14: training_accuracy = 12.01 %, training_loss= 2.31863  ,Validation accuracy = 11.68 ,Validation loss = 2.34786\n",
            "Epoch 15: training_accuracy = 11.98 %, training_loss= 2.31686  ,Validation accuracy = 11.73 ,Validation loss = 2.34643\n",
            "Epoch 16: training_accuracy = 11.90 %, training_loss= 2.31518  ,Validation accuracy = 11.63 ,Validation loss = 2.34506\n",
            "Epoch 17: training_accuracy = 11.85 %, training_loss= 2.31374  ,Validation accuracy = 11.77 ,Validation loss = 2.34401\n",
            "Epoch 18: training_accuracy = 11.80 %, training_loss= 2.31240  ,Validation accuracy = 11.78 ,Validation loss = 2.34302\n",
            "Epoch 19: training_accuracy = 11.79 %, training_loss= 2.31114  ,Validation accuracy = 11.67 ,Validation loss = 2.34213\n",
            "Epoch 20: training_accuracy = 11.69 %, training_loss= 2.30997  ,Validation accuracy = 11.43 ,Validation loss = 2.34131\n",
            "Epoch 21: training_accuracy = 11.70 %, training_loss= 2.30888  ,Validation accuracy = 11.40 ,Validation loss = 2.34060\n",
            "Epoch 22: training_accuracy = 11.67 %, training_loss= 2.30822  ,Validation accuracy = 11.40 ,Validation loss = 2.34030\n",
            "Epoch 23: training_accuracy = 11.67 %, training_loss= 2.30721  ,Validation accuracy = 11.37 ,Validation loss = 2.33975\n",
            "Epoch 24: training_accuracy = 11.65 %, training_loss= 2.30629  ,Validation accuracy = 11.30 ,Validation loss = 2.33928\n",
            "Epoch 25: training_accuracy = 11.63 %, training_loss= 2.30540  ,Validation accuracy = 11.40 ,Validation loss = 2.33871\n",
            "Epoch 26: training_accuracy = 11.58 %, training_loss= 2.30450  ,Validation accuracy = 11.25 ,Validation loss = 2.33823\n",
            "Epoch 27: training_accuracy = 11.53 %, training_loss= 2.30365  ,Validation accuracy = 11.07 ,Validation loss = 2.33771\n",
            "Epoch 28: training_accuracy = 11.52 %, training_loss= 2.30284  ,Validation accuracy = 11.05 ,Validation loss = 2.33720\n",
            "Epoch 29: training_accuracy = 11.42 %, training_loss= 2.30205  ,Validation accuracy = 10.87 ,Validation loss = 2.33677\n",
            "Epoch 30: training_accuracy = 11.35 %, training_loss= 2.30129  ,Validation accuracy = 10.68 ,Validation loss = 2.33640\n",
            "Epoch 31: training_accuracy = 11.30 %, training_loss= 2.30061  ,Validation accuracy = 10.73 ,Validation loss = 2.33604\n",
            "Epoch 32: training_accuracy = 11.29 %, training_loss= 2.29993  ,Validation accuracy = 10.68 ,Validation loss = 2.33578\n",
            "Epoch 33: training_accuracy = 11.26 %, training_loss= 2.29932  ,Validation accuracy = 10.70 ,Validation loss = 2.33549\n",
            "Epoch 34: training_accuracy = 11.31 %, training_loss= 2.29862  ,Validation accuracy = 10.72 ,Validation loss = 2.33522\n",
            "Epoch 35: training_accuracy = 11.27 %, training_loss= 2.29803  ,Validation accuracy = 10.63 ,Validation loss = 2.33499\n",
            "Epoch 36: training_accuracy = 11.23 %, training_loss= 2.29743  ,Validation accuracy = 10.58 ,Validation loss = 2.33486\n",
            "Epoch 37: training_accuracy = 11.16 %, training_loss= 2.29684  ,Validation accuracy = 10.62 ,Validation loss = 2.33467\n",
            "Epoch 38: training_accuracy = 11.07 %, training_loss= 2.29633  ,Validation accuracy = 10.82 ,Validation loss = 2.33448\n",
            "Epoch 39: training_accuracy = 11.02 %, training_loss= 2.29587  ,Validation accuracy = 10.75 ,Validation loss = 2.33439\n",
            "Epoch 40: training_accuracy = 10.94 %, training_loss= 2.29531  ,Validation accuracy = 10.73 ,Validation loss = 2.33415\n",
            "Epoch 41: training_accuracy = 10.88 %, training_loss= 2.29480  ,Validation accuracy = 10.50 ,Validation loss = 2.33401\n",
            "Epoch 42: training_accuracy = 10.81 %, training_loss= 2.29434  ,Validation accuracy = 10.42 ,Validation loss = 2.33390\n",
            "Epoch 43: training_accuracy = 10.81 %, training_loss= 2.29387  ,Validation accuracy = 10.52 ,Validation loss = 2.33361\n",
            "Epoch 44: training_accuracy = 10.76 %, training_loss= 2.29341  ,Validation accuracy = 10.42 ,Validation loss = 2.33354\n",
            "Epoch 45: training_accuracy = 10.70 %, training_loss= 2.29296  ,Validation accuracy = 10.42 ,Validation loss = 2.33336\n",
            "Epoch 46: training_accuracy = 10.65 %, training_loss= 2.29252  ,Validation accuracy = 10.35 ,Validation loss = 2.33330\n",
            "Epoch 47: training_accuracy = 10.66 %, training_loss= 2.29209  ,Validation accuracy = 10.38 ,Validation loss = 2.33334\n",
            "Epoch 48: training_accuracy = 10.64 %, training_loss= 2.29167  ,Validation accuracy = 10.38 ,Validation loss = 2.33325\n",
            "Epoch 49: training_accuracy = 10.61 %, training_loss= 2.29131  ,Validation accuracy = 10.35 ,Validation loss = 2.33317\n",
            "Test accuracy:  10.65\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}